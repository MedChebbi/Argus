{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "line_classification.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVBRu-jgCtDt",
        "outputId": "0c0f8a7f-6d25-4702-ae50-33e83e469682"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHR4e2A3Myha"
      },
      "source": [
        "#Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOwAcTCtORrg",
        "outputId": "b2f557e0-5553-4138-df1d-c7143c542245"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaNNjAFOLEH"
      },
      "source": [
        "import zipfile\n",
        "path=\"drive/MyDrive/\"\n",
        "zip_ref = zipfile.ZipFile(path+\"line_dataset.zip\", 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXuoSRZhMyEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7d951a-6e4a-49c1-db86-a15f679c52e4"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen=ImageDataGenerator(rescale=1/255,\n",
        "                                 rotation_range = 5,\n",
        "                                 shear_range = 0.03,\n",
        "                                 zoom_range = [0.98,1.1],\n",
        "                                 horizontal_flip = False,\n",
        "                                 vertical_flip = False,\n",
        "                                 validation_split=0.2)\n",
        "\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "        \"/content/line_dataset\",\n",
        "        target_size=(64,64),\n",
        "        color_mode='grayscale',\n",
        "        class_mode='categorical',\n",
        "        batch_size=4,\n",
        "        shuffle= True,\n",
        "        subset='training')\n",
        "test_set = train_datagen.flow_from_directory(\n",
        "        \"/content/line_dataset\",\n",
        "        target_size=(64,64),\n",
        "        color_mode='grayscale',\n",
        "        batch_size=4,\n",
        "        class_mode='categorical',\n",
        "        shuffle= True,\n",
        "        subset='validation')\n",
        "\n",
        "print(train_set.class_indices)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 638 images belonging to 6 classes.\n",
            "Found 156 images belonging to 6 classes.\n",
            "{'0_straight': 0, '1_x': 1, '2_T': 2, '3_left': 3, '4_right': 4, '5_end': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owk-f41mWcmN"
      },
      "source": [
        "GRAY_INPUT_SHAPE = (64, 64, 1)\n",
        "NUM_CLASSES = 6\n",
        "class_names = ['straight', 'x', 'T', 'left', 'right', 'end']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhGAvlFIaJmi"
      },
      "source": [
        "def preprocess(img,input_shape=GRAY_INPUT_SHAPE):\n",
        "    img = cv2.resize(img, (input_shape[0],input_shape[1]))\n",
        "    if input_shape[2]==1:\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    img = np.array(np.reshape(img,input_shape),\n",
        "                                      dtype=np.uint8)\n",
        "    \n",
        "    img_test = (np.expand_dims(img, axis = 0))\n",
        "    img_test = img_test/255.\n",
        "    return img_test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAbCIfrCOykn"
      },
      "source": [
        "#A simple grayscale CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93JbuE9fO8wO"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "model_gray= Sequential()\n",
        "model_gray.add(Conv2D(32, (3, 3), input_shape = GRAY_INPUT_SHAPE, activation = 'relu' )) \n",
        "model_gray.add(MaxPooling2D(pool_size=(2,2))) \n",
        "model_gray.add(BatchNormalization()) \n",
        "\n",
        "model_gray.add(Conv2D(64, (3, 3), activation = 'relu')) \n",
        "model_gray.add(MaxPooling2D(pool_size = (2, 2))) \n",
        "model_gray.add(BatchNormalization())\n",
        "\n",
        "model_gray.add(Conv2D(128, (3, 3), activation = 'relu')) \n",
        "model_gray.add(MaxPooling2D(pool_size = (2, 2))) \n",
        "model_gray.add(BatchNormalization())\n",
        "\n",
        "model_gray.add(Flatten())\n",
        "model_gray.add(Dense(units = 64, activation = 'relu'))  \n",
        "model_gray.add(Dropout( rate = 0.2 ))\n",
        "model_gray.add(Dense(units = 32, activation = 'relu'))  \n",
        "model_gray.add(Dense(units = 16, activation = 'relu'))  \n",
        "\n",
        "model_gray.add(Dense(units = NUM_CLASSES , activation = 'softmax')) "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RatZMJ6v1U1M",
        "outputId": "c9b33dca-a886-448f-f99f-58bbe0477e67"
      },
      "source": [
        "model_gray.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 62, 62, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 31, 31, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 29, 29, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 6, 6, 128)         512       \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                294976    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 391,254\n",
            "Trainable params: 390,806\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cpJz262YFA_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2705f9cd-ebc2-46bd-a7df-cc952aac54cf"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "early = EarlyStopping(monitor='val_accuracy', \n",
        "                      min_delta=0, \n",
        "                      patience=8, \n",
        "                      verbose=1, \n",
        "                      mode='auto')\n",
        "\n",
        "model_gray.compile(optimizer='adam',\n",
        "                   loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "history = model_gray.fit(\n",
        "          train_set,\n",
        "          steps_per_epoch=50,\n",
        "          epochs=30,\n",
        "          validation_data=test_set,\n",
        "          validation_steps=20,\n",
        "          shuffle = True,\n",
        "          callbacks=[early])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "50/50 [==============================] - 4s 53ms/step - loss: 2.2045 - accuracy: 0.2270 - val_loss: 1.9316 - val_accuracy: 0.1125\n",
            "Epoch 2/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 1.7421 - accuracy: 0.4580 - val_loss: 2.5844 - val_accuracy: 0.1000\n",
            "Epoch 3/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 1.1100 - accuracy: 0.6033 - val_loss: 2.9601 - val_accuracy: 0.1000\n",
            "Epoch 4/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 1.2345 - accuracy: 0.5589 - val_loss: 3.6956 - val_accuracy: 0.1375\n",
            "Epoch 5/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 0.8925 - accuracy: 0.6563 - val_loss: 5.1410 - val_accuracy: 0.1875\n",
            "Epoch 6/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.8899 - accuracy: 0.6985 - val_loss: 5.6932 - val_accuracy: 0.1625\n",
            "Epoch 7/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.7441 - accuracy: 0.7694 - val_loss: 4.0913 - val_accuracy: 0.1750\n",
            "Epoch 8/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 0.8039 - accuracy: 0.7294 - val_loss: 2.8680 - val_accuracy: 0.1875\n",
            "Epoch 9/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.4804 - accuracy: 0.8520 - val_loss: 1.3247 - val_accuracy: 0.5000\n",
            "Epoch 10/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 0.5753 - accuracy: 0.7832 - val_loss: 1.2486 - val_accuracy: 0.4250\n",
            "Epoch 11/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.6476 - accuracy: 0.7928 - val_loss: 0.5188 - val_accuracy: 0.7375\n",
            "Epoch 12/30\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.5221 - accuracy: 0.7902 - val_loss: 0.7061 - val_accuracy: 0.7500\n",
            "Epoch 13/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 0.5748 - accuracy: 0.8212 - val_loss: 0.5971 - val_accuracy: 0.8250\n",
            "Epoch 14/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 0.3022 - accuracy: 0.8723 - val_loss: 0.4637 - val_accuracy: 0.8875\n",
            "Epoch 15/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.7036 - accuracy: 0.8213 - val_loss: 0.6411 - val_accuracy: 0.8875\n",
            "Epoch 16/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.2770 - accuracy: 0.9084 - val_loss: 0.3803 - val_accuracy: 0.9125\n",
            "Epoch 17/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.4833 - accuracy: 0.8224 - val_loss: 1.3479 - val_accuracy: 0.5375\n",
            "Epoch 18/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.3797 - accuracy: 0.8721 - val_loss: 0.2607 - val_accuracy: 0.9125\n",
            "Epoch 19/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.3290 - accuracy: 0.8864 - val_loss: 0.5992 - val_accuracy: 0.8750\n",
            "Epoch 20/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.2890 - accuracy: 0.9117 - val_loss: 0.3916 - val_accuracy: 0.9375\n",
            "Epoch 21/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.4468 - accuracy: 0.9004 - val_loss: 0.7297 - val_accuracy: 0.7625\n",
            "Epoch 22/30\n",
            "50/50 [==============================] - 2s 47ms/step - loss: 0.4416 - accuracy: 0.8672 - val_loss: 0.1978 - val_accuracy: 0.9250\n",
            "Epoch 23/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.2649 - accuracy: 0.9305 - val_loss: 0.5677 - val_accuracy: 0.9250\n",
            "Epoch 24/30\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.3400 - accuracy: 0.8853 - val_loss: 0.5703 - val_accuracy: 0.8750\n",
            "Epoch 25/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.3786 - accuracy: 0.8745 - val_loss: 0.3655 - val_accuracy: 0.9625\n",
            "Epoch 26/30\n",
            "50/50 [==============================] - 2s 48ms/step - loss: 0.1888 - accuracy: 0.9485 - val_loss: 0.4704 - val_accuracy: 0.9250\n",
            "Epoch 27/30\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.2623 - accuracy: 0.9262 - val_loss: 0.6437 - val_accuracy: 0.9125\n",
            "Epoch 28/30\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.1940 - accuracy: 0.9128 - val_loss: 0.4075 - val_accuracy: 0.8625\n",
            "Epoch 29/30\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.0895 - accuracy: 0.9780 - val_loss: 0.0400 - val_accuracy: 0.9875\n",
            "Epoch 30/30\n",
            "50/50 [==============================] - 2s 49ms/step - loss: 0.1683 - accuracy: 0.9377 - val_loss: 0.6670 - val_accuracy: 0.8875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6TwDmAKTYAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3800ff-c57b-429e-9c28-a05925831292"
      },
      "source": [
        "history.history"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.2929292917251587,\n",
              "  0.4300000071525574,\n",
              "  0.6161616444587708,\n",
              "  0.5707070827484131,\n",
              "  0.6549999713897705,\n",
              "  0.7350000143051147,\n",
              "  0.7599999904632568,\n",
              "  0.7575757503509521,\n",
              "  0.7979797720909119,\n",
              "  0.7850000262260437,\n",
              "  0.7575757503509521,\n",
              "  0.8450000286102295,\n",
              "  0.8349999785423279,\n",
              "  0.8799999952316284,\n",
              "  0.8349999785423279,\n",
              "  0.8849999904632568,\n",
              "  0.8550000190734863,\n",
              "  0.8349999785423279,\n",
              "  0.875,\n",
              "  0.9049999713897705,\n",
              "  0.925000011920929,\n",
              "  0.8888888955116272,\n",
              "  0.8949999809265137,\n",
              "  0.8949999809265137,\n",
              "  0.8899999856948853,\n",
              "  0.9300000071525574,\n",
              "  0.9399999976158142,\n",
              "  0.9049999713897705,\n",
              "  0.9599999785423279,\n",
              "  0.9300000071525574],\n",
              " 'loss': [2.1708011627197266,\n",
              "  1.6360357999801636,\n",
              "  1.1393519639968872,\n",
              "  1.1816385984420776,\n",
              "  0.944318413734436,\n",
              "  0.840807318687439,\n",
              "  0.7043262720108032,\n",
              "  0.6870132684707642,\n",
              "  0.5610242486000061,\n",
              "  0.6145662069320679,\n",
              "  0.7166757583618164,\n",
              "  0.41420093178749084,\n",
              "  0.4355483949184418,\n",
              "  0.3130492866039276,\n",
              "  0.6867541670799255,\n",
              "  0.3205755352973938,\n",
              "  0.39196422696113586,\n",
              "  0.46869537234306335,\n",
              "  0.36127203702926636,\n",
              "  0.30450838804244995,\n",
              "  0.3467961847782135,\n",
              "  0.3016816973686218,\n",
              "  0.3216007947921753,\n",
              "  0.3621358871459961,\n",
              "  0.3359735906124115,\n",
              "  0.24874430894851685,\n",
              "  0.19509989023208618,\n",
              "  0.22473034262657166,\n",
              "  0.10595529526472092,\n",
              "  0.21335679292678833],\n",
              " 'val_accuracy': [0.11249999701976776,\n",
              "  0.10000000149011612,\n",
              "  0.10000000149011612,\n",
              "  0.13750000298023224,\n",
              "  0.1875,\n",
              "  0.16249999403953552,\n",
              "  0.17499999701976776,\n",
              "  0.1875,\n",
              "  0.5,\n",
              "  0.42500001192092896,\n",
              "  0.737500011920929,\n",
              "  0.75,\n",
              "  0.824999988079071,\n",
              "  0.887499988079071,\n",
              "  0.887499988079071,\n",
              "  0.9125000238418579,\n",
              "  0.5375000238418579,\n",
              "  0.9125000238418579,\n",
              "  0.875,\n",
              "  0.9375,\n",
              "  0.762499988079071,\n",
              "  0.925000011920929,\n",
              "  0.925000011920929,\n",
              "  0.875,\n",
              "  0.9624999761581421,\n",
              "  0.925000011920929,\n",
              "  0.9125000238418579,\n",
              "  0.862500011920929,\n",
              "  0.987500011920929,\n",
              "  0.887499988079071],\n",
              " 'val_loss': [1.9316343069076538,\n",
              "  2.5844051837921143,\n",
              "  2.9601263999938965,\n",
              "  3.6955952644348145,\n",
              "  5.1410417556762695,\n",
              "  5.693243980407715,\n",
              "  4.091250419616699,\n",
              "  2.8680291175842285,\n",
              "  1.3246937990188599,\n",
              "  1.248643159866333,\n",
              "  0.5188461542129517,\n",
              "  0.7060758471488953,\n",
              "  0.5970983505249023,\n",
              "  0.4637066423892975,\n",
              "  0.6410791873931885,\n",
              "  0.3803180158138275,\n",
              "  1.3478549718856812,\n",
              "  0.26065751910209656,\n",
              "  0.5992468595504761,\n",
              "  0.39157670736312866,\n",
              "  0.7296541929244995,\n",
              "  0.19777636229991913,\n",
              "  0.5676907300949097,\n",
              "  0.5703170895576477,\n",
              "  0.3655014634132385,\n",
              "  0.4703855514526367,\n",
              "  0.6437286138534546,\n",
              "  0.4074546694755554,\n",
              "  0.04001161828637123,\n",
              "  0.6669647693634033]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxCzblajWNe4",
        "outputId": "d948fd39-3e5d-41cd-9ad7-d331c0508a3d"
      },
      "source": [
        "results = model_gray.evaluate(test_set, batch_size=4)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 1s 18ms/step - loss: 0.4354 - accuracy: 0.9167\n",
            "test loss, test acc: [0.43537184596061707, 0.9166666865348816]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "K7PN5U_GXdOt",
        "outputId": "36d68fef-e7bb-4ffe-9bfe-3bc2759aaf97"
      },
      "source": [
        "img = cv2.imread('end2.jpg')\n",
        "\n",
        "test_img = preprocess(img)\n",
        "print(test_img.shape)\n",
        "pyplot.imshow(img)\n",
        "prediction = model_gray.predict(test_img)\n",
        "print(prediction)\n",
        "pred_index = np.argmax(prediction[0])\n",
        "print('The predicted class is:', class_names[pred_index] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 64, 1)\n",
            "[[7.04735667e-02 5.61234128e-06 6.82490572e-05 1.16130595e-05\n",
            "  1.03129787e-04 9.29337800e-01]]\n",
            "The predicted class is: end\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz7A7wS1Z6Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "589ba1c1-984b-46f1-8b4f-7cfdd87a7afa"
      },
      "source": [
        "model_gray.save(\"drive/MyDrive/grayscale_line_classifier\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: drive/MyDrive/grayscale_line_classifier/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMHP70OMrq6"
      },
      "source": [
        "#Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNNYO57GC6xV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ce5f77-495f-4810-b417-7be3c4b5e4b4"
      },
      "source": [
        "INPUT_SHAPE = (128, 128, 3)\n",
        "NUM_CLASSES = 6\n",
        "\n",
        "base_model = keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    pooling=None)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0KWBA6B4TkM",
        "outputId": "81b09663-23d3-4a7e-b596-c98cc071eac2"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen_rgb=ImageDataGenerator(rescale=1/255,\n",
        "                                 rotation_range = 5,\n",
        "                                 shear_range = 0.03,\n",
        "                                 zoom_range = [0.98,1.1],\n",
        "                                 horizontal_flip = False,\n",
        "                                 vertical_flip = False,\n",
        "                                 validation_split=0.2)\n",
        "\n",
        "train_set_rgb = train_datagen_rgb.flow_from_directory(\n",
        "        \"/content/line_dataset\",\n",
        "        target_size=(128,128),\n",
        "        color_mode='rgb',\n",
        "        class_mode='categorical',\n",
        "        batch_size=4,\n",
        "        shuffle= True,\n",
        "        subset='training')\n",
        "test_set_rgb = train_datagen.flow_from_directory(\n",
        "        \"/content/line_dataset\",\n",
        "        target_size=(128,128),\n",
        "        color_mode='rgb',\n",
        "        batch_size=4,\n",
        "        class_mode='categorical',\n",
        "        shuffle= True,\n",
        "        subset='validation')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 638 images belonging to 6 classes.\n",
            "Found 156 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ibj1eKtE7fD",
        "outputId": "c60bad0d-6322-4909-acb6-abf22d6eff4d"
      },
      "source": [
        "base_model.trainable = False\n",
        "#base_model.summary()\n",
        "inputs = keras.Input(shape=INPUT_SHAPE)\n",
        "\n",
        "x = base_model(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "# A Dense classifier with a single unit (binary classification)\n",
        "flat1 = keras.layers.Dense(1024, activation='relu')(x)\n",
        "outputs = keras.layers.Dense(NUM_CLASSES,activation='softmax')(flat1)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 4, 4, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 25,692,038\n",
            "Trainable params: 2,104,326\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CTwMQyhFGal",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed59b502-ae1f-45a3-e2e6-8cb128b4deec"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "early = EarlyStopping(monitor='val_accuracy', \n",
        "                      min_delta=0, \n",
        "                      patience=8, \n",
        "                      verbose=1, \n",
        "                      mode='auto')\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "          train_set_rgb,\n",
        "          steps_per_epoch=60,\n",
        "          epochs=35,\n",
        "          validation_data=test_set_rgb,\n",
        "          validation_steps=20,\n",
        "          shuffle = True,\n",
        "          callbacks=[early])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "60/60 [==============================] - 26s 364ms/step - loss: 1.8954 - accuracy: 0.2694 - val_loss: 1.3548 - val_accuracy: 0.5000\n",
            "Epoch 2/35\n",
            "60/60 [==============================] - 21s 343ms/step - loss: 1.1254 - accuracy: 0.6130 - val_loss: 0.8406 - val_accuracy: 0.6500\n",
            "Epoch 3/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.7591 - accuracy: 0.7836 - val_loss: 0.7222 - val_accuracy: 0.7875\n",
            "Epoch 4/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.5889 - accuracy: 0.8527 - val_loss: 0.6040 - val_accuracy: 0.8000\n",
            "Epoch 5/35\n",
            "60/60 [==============================] - 21s 344ms/step - loss: 0.5136 - accuracy: 0.8563 - val_loss: 0.7004 - val_accuracy: 0.7500\n",
            "Epoch 6/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.4824 - accuracy: 0.8276 - val_loss: 0.4826 - val_accuracy: 0.8625\n",
            "Epoch 7/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.4213 - accuracy: 0.8879 - val_loss: 0.3580 - val_accuracy: 0.9000\n",
            "Epoch 8/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.3249 - accuracy: 0.9253 - val_loss: 0.4790 - val_accuracy: 0.8500\n",
            "Epoch 9/35\n",
            "60/60 [==============================] - 20s 340ms/step - loss: 0.3023 - accuracy: 0.9113 - val_loss: 0.4021 - val_accuracy: 0.9000\n",
            "Epoch 10/35\n",
            "60/60 [==============================] - 20s 339ms/step - loss: 0.2131 - accuracy: 0.9521 - val_loss: 0.3532 - val_accuracy: 0.9375\n",
            "Epoch 11/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.2491 - accuracy: 0.9303 - val_loss: 0.3811 - val_accuracy: 0.9000\n",
            "Epoch 12/35\n",
            "60/60 [==============================] - 20s 343ms/step - loss: 0.1865 - accuracy: 0.9626 - val_loss: 0.2497 - val_accuracy: 0.9250\n",
            "Epoch 13/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.1666 - accuracy: 0.9765 - val_loss: 0.2637 - val_accuracy: 0.9125\n",
            "Epoch 14/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.1412 - accuracy: 0.9718 - val_loss: 0.4180 - val_accuracy: 0.8625\n",
            "Epoch 15/35\n",
            "60/60 [==============================] - 20s 340ms/step - loss: 0.2299 - accuracy: 0.9021 - val_loss: 0.3099 - val_accuracy: 0.9125\n",
            "Epoch 16/35\n",
            "60/60 [==============================] - 20s 342ms/step - loss: 0.1451 - accuracy: 0.9639 - val_loss: 0.3801 - val_accuracy: 0.8875\n",
            "Epoch 17/35\n",
            "60/60 [==============================] - 20s 340ms/step - loss: 0.1152 - accuracy: 0.9700 - val_loss: 0.4921 - val_accuracy: 0.8625\n",
            "Epoch 18/35\n",
            "60/60 [==============================] - 21s 347ms/step - loss: 0.1164 - accuracy: 0.9619 - val_loss: 0.4626 - val_accuracy: 0.9125\n",
            "Epoch 00018: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKRa2oFV7yUS",
        "outputId": "0bba0ced-d5f3-4acf-9072-922c659d5117"
      },
      "source": [
        "results = model.evaluate(test_set_rgb, batch_size=4)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 10s 252ms/step - loss: 0.2653 - accuracy: 0.9487\n",
            "test loss, test acc: [0.26528504490852356, 0.9487179517745972]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38aTFxW4PUYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83c33b5-611f-48cf-c02a-a019cd29ef8c"
      },
      "source": [
        "model.save(\"drive/MyDrive/resnet50_line_classifier\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: drive/MyDrive/resnet50_line_classifier/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKJEGnVWM5tT"
      },
      "source": [
        "#Optimizing and converting model to tfLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Eae2CfR88s"
      },
      "source": [
        "saved_model_dir = \"drive/MyDrive/resnet50_line_classifier\""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcCDe34Sb-E"
      },
      "source": [
        "##For CPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piUQd_7eNC8f"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # path to the SavedModel directory\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model_resnet50.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9EkVrFYRxE4"
      },
      "source": [
        "##For edge TPU compatibility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yND6BOJJRiFz"
      },
      "source": [
        "def representative_dataset():\n",
        "  for data in tf.data.Dataset.from_tensor_slices((images)).batch(1).take(100):\n",
        "    yield [tf.dtypes.cast(data, tf.float32)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7l9PzQSRm6F"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ7B2w3nNDzS"
      },
      "source": [
        "#Testing tfLite model inference with tfLite interpretter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "nvrm31siwdZT",
        "outputId": "a427a1b4-50ed-431e-f3b4-ce45318292e5"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "#set random np array to test\n",
        "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "\n",
        "img = cv2.imread('straight1.jpg')\n",
        "\n",
        "test_img = preprocess(img)\n",
        "pyplot.imshow(img)\n",
        "\n",
        "input_data = np.array(test_img, dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)\n",
        "pred_index = np.argmax(output_data[0])\n",
        "print('The predicted class is:', class_names[pred_index] )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.7342328e-01 4.7582256e-05 6.7231787e-10 5.6731026e-04 3.9084390e-04\n",
            "  1.2557094e-01]]\n",
            "The predicted class is: straight\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMcmLFbPCtO"
      },
      "source": [
        "#Comparing the two models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w1hNm_2PB4N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}